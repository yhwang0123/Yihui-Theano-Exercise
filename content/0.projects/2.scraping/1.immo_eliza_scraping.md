# Collecting Data

- Repository: `challenge-collecting-data`
- Type of Challenge: `Consolidation`
- Duration: `3 days`
- Deadline: `03/06/2022 4:00 PM`
- Project type : `Individual`

## Learning objectives

Use a python library to collect as much data as possible.

At the end of this challenge, you should :

- Be able to scrape a website
- Be able to build a dataset from scratch
- Be able to implement a strategy to collect as much data as possible.

## The Mission

The real estate company "ImmoEliza" wants to create a machine learning model to make price predictions on real estate sales in Belgium. You must therefore create a dataset that holds the following columns :

- Locality
- Type of property (House/apartment)
- Subtype of property (Bungalow, Chalet, Mansion, ...)
- Price
- Type of sale (Exclusion of life sales)
- Number of rooms
- Area
- Fully equipped kitchen (Yes/No)
- Furnished (Yes/No)
- Open fire (Yes/No)
- Terrace (Yes/No)
  - If yes: Area
- Garden (Yes/No)
  - If yes: Area
- Surface of the land
- Surface area of the plot of land
- Number of facades
- Swimming pool (Yes/No)
- State of the building (New, to be renovated, ...)

You must save everything in a `.csv` file.

### Must-have features

- Data for all of Belgium.
- Minimum 10 000 inputs
- No empty row. If you are missing information, set the value to `None`.

### Miscellaneous information / advices

- Be careful not to have duplicates. The dataset must be clean. Try as much as possible to record only numerical values (e.g.: instead of defining whether the kitchen is equipped using `"Yes"`, use binary values).

- All fields are not always present. You must pay attention to missing values and implement a strategy to handle them in an efficient way.

- Try to use threading to increase the speed of data collection. Be mindful of what needs to be in a thread and what is not necessary.

- You might have to work around CAPTCHA and other measures to slow you down. Be creative ;)

- Make sure to have a code working for one page before trying to fetch 10000 pages in one shot !

## Deliverables

1. Publish your source code on a GitHub repository.
2. Pimp up the README file:
   - Description
   - Installation
   - Usage
   - (Visuals)
   - (Contributors)
   - (Timeline)
   - (Personal situation)
3. Small presentation :
   - How you did it ?
   - Who did what ?
   - What went wrong ?
   - How you solved it ?

### Steps

1. Create the repository
2. Study the request (What & Why ?)
3. Identify technical challenges (How ?)

## Evaluation criteria

| Criteria       | Indicator                                  | Yes/No |
| -------------- | ------------------------------------------ | ------ |
| 1. Is complete | Contains a minimum of 10,000 inputs.       | [ ]    |
|                | Contains data for all of Belgium.          | [ ]    |
|                | No empty row present in the dataset.       | [ ]    |
|                | Non-numeric values have been minimized.    | [ ]    |
| 2. Is great    | Used threading to speed up the collection. | [ ]    |

## A final note of encouragement

_Attempts to create thinking machines will be a great help in discovering how we think ourselves._
_- Alan Turing_

![You've got this!](https://media.giphy.com/media/11Xq4vVmbFCHhS/giphy.gif)
